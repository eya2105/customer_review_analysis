{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07643067",
   "metadata": {},
   "source": [
    "# Preprocessing for classical ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c05650a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291f6193",
   "metadata": {},
   "source": [
    "## Directories and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0bf920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINETUNING_SPLITS_DIR = \"../data/cleaned/finetuning-splits/\"\n",
    "ML_SPLITS_DIR = \"../data/cleaned/ml-methods-splits/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc58d6d7",
   "metadata": {},
   "source": [
    "## NLTK stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e8a71bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\OrdiOne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "french_stop = set(stopwords.words(\"french\"))\n",
    "negations = {\"ne\", \"pas\", \"jamais\", \"rien\", \"aucun\", \"sans\", \"not\", \"no\", \"never\", \"none\"}\n",
    "french_stop = french_stop - negations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385fc48d",
   "metadata": {},
   "source": [
    "## Load train/test splits from fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0019914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading stratified train/test splits from fine-tuning notebook...\n",
      "Train: (433, 2), Test: (109, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading stratified train/test splits from fine-tuning notebook...\")\n",
    "train_df = pd.read_csv(f\"{FINETUNING_SPLITS_DIR}/train_set.csv\")\n",
    "test_df  = pd.read_csv(f\"{FINETUNING_SPLITS_DIR}/test_set.csv\")\n",
    "print(f\"Train: {train_df.shape}, Test: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07941b44",
   "metadata": {},
   "source": [
    "## Text cleaning function for classic ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f045059b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_ml(text):\n",
    "    \"\"\"Clean text for TF-IDF / classical ML.\"\"\"\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # emoji mapping\n",
    "    emoji_replacements = {\n",
    "        \"üòä\": \" _emoji_souriant_ \",\n",
    "        \"üòç\": \" _emoji_coeur_ \",\n",
    "        \"üëç\": \" _emoji_ok_ \",\n",
    "        \"üëé\": \" _emoji_pas_ok_ \",\n",
    "        \"üò†\": \" _emoji_enerve_ \",\n",
    "        \"üòî\": \" _emoji_triste_ \",\n",
    "        \"‚≠ê\": \" _emoji_etoile_ \",\n",
    "        \"üåü\": \" _emoji_etoile_brillante_ \",\n",
    "    }\n",
    "    for emoji, replacement in emoji_replacements.items():\n",
    "        text = text.replace(emoji, replacement)\n",
    "    \n",
    "    # remove unwanted chars but keep basic punctuation\n",
    "    text = re.sub(r\"[^\\w\\s√†√¢√§√©√®√™√´√Æ√Ø√¥√∂√π√ª√º√ß.!?,;:]\", \" \", text)\n",
    "    \n",
    "    # mark punctuation\n",
    "    text = re.sub(r\"(!)\", \" _exclamation_ \", text)\n",
    "    text = re.sub(r\"(\\?)\", \" _question_ \", text)\n",
    "    \n",
    "    # remove URLs, mentions, hashtags\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \" \", text)\n",
    "    text = re.sub(r\"@\\w+\", \" \", text)\n",
    "    text = re.sub(r\"#\\w+\", \" \", text)\n",
    "    \n",
    "    # normalize spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1260c9d3",
   "metadata": {},
   "source": [
    "## Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c630b250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    return \" \".join([w for w in words if w not in french_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc47588",
   "metadata": {},
   "source": [
    "## Complete preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c260806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataframe(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Step 1: clean text\n",
    "    df[\"text_clean\"] = df[\"text\"].apply(clean_text_ml)\n",
    "    \n",
    "    # Step 2: remove stopwords\n",
    "    df[\"text_clean\"] = df[\"text_clean\"].apply(remove_stopwords)\n",
    "    \n",
    "    \n",
    "    # Step 3: remove empty rows after cleaning\n",
    "    before = df.shape[0]\n",
    "    df[\"text_clean\"] = df[\"text_clean\"].replace(\"\", np.nan)\n",
    "    df = df.dropna(subset=[\"text_clean\"])\n",
    "    after = df.shape[0]\n",
    "    print(f\"Removed {before - after} empty rows after preprocessing.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f824012",
   "metadata": {},
   "source": [
    "## Apply preprocessing on train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abf6b62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing train set...\n",
      "Removed 2 empty rows after preprocessing.\n",
      "\n",
      "Preprocessing test set...\n",
      "Removed 4 empty rows after preprocessing.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPreprocessing train set...\")\n",
    "train_df = preprocess_dataframe(train_df)\n",
    "\n",
    "print(\"\\nPreprocessing test set...\")\n",
    "test_df = preprocess_dataframe(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c01285",
   "metadata": {},
   "source": [
    "## Distribution of data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "826b8e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Train: 431 samples\n",
      "   Test:  105 samples\n",
      "Distribution of the training data :  label\n",
      "2    298\n",
      "0     94\n",
      "1     39\n",
      "Name: count, dtype: int64\n",
      "Distribution of the testing data :  label\n",
      "2    71\n",
      "0    24\n",
      "1    10\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_train = train_df['text_clean']\n",
    "y_train = train_df['label']\n",
    "X_test = test_df['text_clean']\n",
    "y_test = test_df['label']\n",
    "print(f\"   Train: {len(X_train)} samples\")\n",
    "print(f\"   Test:  {len(X_test)} samples\")\n",
    "print(f'Distribution of the training data : ',y_train.value_counts())\n",
    "print(f'Distribution of the testing data : ',y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507d3a11",
   "metadata": {},
   "source": [
    "## Save preprocessed splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c428baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train saved to: ../data/cleaned/ml-methods-splits//train_set.csv ‚Äî shape: (431, 3)\n",
      "Test saved  to: ../data/cleaned/ml-methods-splits//test_set.csv ‚Äî shape: (105, 3)\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(ML_SPLITS_DIR, exist_ok=True)\n",
    "train_path = f\"{ML_SPLITS_DIR}/train_set.csv\"\n",
    "test_path  = f\"{ML_SPLITS_DIR}/test_set.csv\"\n",
    "train_df[[\"text_clean\", \"label\"]].to_csv(train_path, index=False, encoding=\"utf-8-sig\")\n",
    "test_df[[\"text_clean\", \"label\"]].to_csv(test_path,  index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"Train saved to: {train_path} ‚Äî shape: {train_df.shape}\")\n",
    "print(f\"Test saved  to: {test_path} ‚Äî shape: {test_df.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

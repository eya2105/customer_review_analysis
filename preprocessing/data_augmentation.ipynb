{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T18:45:38.203857Z",
     "iopub.status.busy": "2025-12-07T18:45:38.203123Z",
     "iopub.status.idle": "2025-12-07T18:45:38.549993Z",
     "shell.execute_reply": "2025-12-07T18:45:38.549400Z",
     "shell.execute_reply.started": "2025-12-07T18:45:38.203830Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /usr/share/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from deep_translator import GoogleTranslator\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from nlpaug.augmenter.word import SynonymAug\n",
    "import nltk\n",
    "\n",
    "# Fix NLTK resources required by SynonymAug\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-07T18:45:39.643133Z",
     "iopub.status.busy": "2025-12-07T18:45:39.642882Z",
     "iopub.status.idle": "2025-12-07T18:45:39.652738Z",
     "shell.execute_reply": "2025-12-07T18:45:39.651991Z",
     "shell.execute_reply.started": "2025-12-07T18:45:39.643117Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class counts: label\n",
      "2    298\n",
      "0     94\n",
      "1     39\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "ORIG_CSV =\"../data/cleaned/ml-methods-splits/train_set.csv\"\n",
    "OUTPUT_DIR =\"../data/cleaned/ml-methods-splits/augmented_simple\"\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "df = pd.read_csv(ORIG_CSV, encoding=\"utf-8-sig\")\n",
    "print(\"Original class counts:\", df['label'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T18:45:40.037799Z",
     "iopub.status.busy": "2025-12-07T18:45:40.037271Z",
     "iopub.status.idle": "2025-12-07T18:45:40.977389Z",
     "shell.execute_reply": "2025-12-07T18:45:40.976802Z",
     "shell.execute_reply.started": "2025-12-07T18:45:40.037780Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "embed_model = SentenceTransformer('dangvantuan/sentence-camembert-base')\n",
    "syn_aug = SynonymAug(aug_src='wordnet', lang='fra', aug_p=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T18:45:40.978862Z",
     "iopub.status.busy": "2025-12-07T18:45:40.978512Z",
     "iopub.status.idle": "2025-12-07T18:45:40.983854Z",
     "shell.execute_reply": "2025-12-07T18:45:40.983162Z",
     "shell.execute_reply.started": "2025-12-07T18:45:40.978838Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def back_translate_fr(text, pivot='en'):\n",
    "    try:\n",
    "        inter = GoogleTranslator(source='fr', target=pivot).translate(text)\n",
    "        if not inter or len(inter.strip()) < 3:\n",
    "            return None\n",
    "        back = GoogleTranslator(source=pivot, target='fr').translate(inter)\n",
    "        if not back or back.strip() == \"\" or back.strip() == text.strip():\n",
    "            return None\n",
    "        return back\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T18:45:40.984612Z",
     "iopub.status.busy": "2025-12-07T18:45:40.984429Z",
     "iopub.status.idle": "2025-12-07T18:45:41.004035Z",
     "shell.execute_reply": "2025-12-07T18:45:41.003490Z",
     "shell.execute_reply.started": "2025-12-07T18:45:40.984588Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def is_semantic_sim(orig, aug, threshold=0.80):\n",
    "    emb_o = embed_model.encode(orig, convert_to_tensor=True)\n",
    "    emb_a = embed_model.encode(aug, convert_to_tensor=True)\n",
    "    return util.cos_sim(emb_o, emb_a).item() >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T18:45:42.508389Z",
     "iopub.status.busy": "2025-12-07T18:45:42.507893Z",
     "iopub.status.idle": "2025-12-07T18:45:42.513263Z",
     "shell.execute_reply": "2025-12-07T18:45:42.512575Z",
     "shell.execute_reply.started": "2025-12-07T18:45:42.508368Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def augment_text(text, n_aug=1):\n",
    "    \"\"\"Generate up to n_aug augmentations from one text.\"\"\"\n",
    "    augmented = []\n",
    "    # try back‑translation\n",
    "    for pivot in ['en', 'de', 'es']:\n",
    "        if len(augmented) >= n_aug:\n",
    "            break\n",
    "        bt = back_translate_fr(text, pivot)\n",
    "        if bt and is_semantic_sim(text, bt, threshold=0.80):\n",
    "            augmented.append(bt)\n",
    "    # if still need more and synonyms allowed\n",
    "    while len(augmented) < n_aug:\n",
    "        syn = syn_aug.augment(text)\n",
    "        if syn and syn != text and is_semantic_sim(text, syn, threshold=0.90):\n",
    "            augmented.append(syn)\n",
    "        else:\n",
    "            break\n",
    "    return augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T19:00:33.267311Z",
     "iopub.status.busy": "2025-12-07T19:00:33.267110Z",
     "iopub.status.idle": "2025-12-07T19:02:10.584778Z",
     "shell.execute_reply": "2025-12-07T19:02:10.584164Z",
     "shell.execute_reply.started": "2025-12-07T19:00:33.267294Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented class counts: label\n",
      "2    275\n",
      "0    247\n",
      "1    201\n",
      "Name: count, dtype: int64\n",
      "Saved augmented data to: /kaggle/working/train_augmented_simple.csv\n"
     ]
    }
   ],
   "source": [
    "# --- BALANCING / AUGMENTATION ---  \n",
    "\n",
    "TARGET = {0: 250, 1: 250, 2: 250}  \n",
    "\n",
    "augmented_rows = []\n",
    "for lbl, group in df.groupby('label'):\n",
    "    subset = group.reset_index(drop=True)\n",
    "    tgt = TARGET.get(lbl, len(subset))\n",
    "    curr = len(subset)\n",
    "    for _, row in subset.iterrows():\n",
    "        augmented_rows.append({'text_clean': row['text_clean'], 'label': lbl, 'is_aug': False})\n",
    "    if curr < tgt:\n",
    "        needed = tgt - curr\n",
    "        per_sample = max(1, needed // curr + 1)\n",
    "        for _, row in subset.iterrows():\n",
    "            aug_texts = augment_text(row['text_clean'], n_aug=per_sample)\n",
    "            for a in aug_texts:\n",
    "                augmented_rows.append({'text_clean': a, 'label': lbl, 'is_aug': True})\n",
    "\n",
    "df_aug = pd.DataFrame(augmented_rows).drop_duplicates(subset=['text_clean']).reset_index(drop=True)\n",
    "print(\"Augmented class counts:\", df_aug['label'].value_counts())\n",
    "\n",
    "out_path = \"../data/cleaned/ml-methods-splits/train_augmented_simple.csv\"\n",
    "df_aug.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Saved augmented data to:\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T19:09:43.885602Z",
     "iopub.status.busy": "2025-12-07T19:09:43.885334Z",
     "iopub.status.idle": "2025-12-07T19:09:43.893653Z",
     "shell.execute_reply": "2025-12-07T19:09:43.893045Z",
     "shell.execute_reply.started": "2025-12-07T19:09:43.885581Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label 0:\n",
      "Original length: 251 chars\n",
      "Text: ne répondez jamais au téléphone _exclamation_ je suis allé au magasin pendant les heures d'ouverture fermées _exclamation_ j'ai laissé un message à la...\n",
      "\n",
      "Label 0:\n",
      "Original length: 244 chars\n",
      "Text: ne réponds jamais au téléphone _exclamation_ est entré dans le magasin pendant les heures d'ouverture fermées _exclamation_ a laissé un mot sur la por...\n",
      "\n",
      "Label 0:\n",
      "Original length: 166 chars\n",
      "Text: vraiment trop bruyant on entend tout voisinage de l'autre côté de la rue chambre pas normale non équipée de système d'insonorisation correct. bref qua...\n",
      "\n",
      "Label 0:\n",
      "Original length: 170 chars\n",
      "Text: vraiment trop fort on entend n'importe quel quartier de l'autre côté de la rue pas une pièce normale non équipée du bon système d'insonorisation. bref...\n",
      "\n",
      "Label 0:\n",
      "Original length: 14 chars\n",
      "Text: _emoji_not_ok_...\n",
      "\n",
      "Label 0:\n",
      "Original length: 342 chars\n",
      "Text: Cette salle de sport a une atmosphère très désagréable à cause de clients peu recommandables. toujours bondé, en dehors des heures de pointe. machines...\n",
      "\n",
      "Label 0:\n",
      "Original length: 351 chars\n",
      "Text: Il y a une atmosphère très désagréable dans cette salle de sport à cause des clients peu recommandables. Toujours occupé en dehors des heures de point...\n",
      "\n",
      "Label 0:\n",
      "Original length: 539 chars\n",
      "Text: vacances marsa mari vit à paris choisi gymnase californien faire du sport. Expérience très décevante. dès l'entrée, coach maigre, lunettes qui fixent ...\n",
      "\n",
      "Label 0:\n",
      "Original length: 599 chars\n",
      "Text: Le mari de Marsa en vacances vit à Paris et a choisi une salle de sport californienne pour faire du sport. Expérience très décevante. En entrant, le c...\n",
      "\n",
      "Label 0:\n",
      "Original length: 26 chars\n",
      "Text: document larguer quand ici...\n",
      "\n",
      "Label 0:\n",
      "Original length: 172 chars\n",
      "Text: Des machines de techno gym, tellement vieilles, sales, souvent en panne. n'a pas de machines à variétés. Douches pas propres. rapport prix, ça ne vaut...\n",
      "\n",
      "Label 0:\n",
      "Original length: 167 chars\n",
      "Text: Matériel de fitness techno, tellement vieux, sale, souvent cassé. n'a pas de machines à variétés. Douches pas propres. Le rapport prix n'en vaut vraim...\n",
      "\n",
      "Label 0:\n",
      "Original length: 1229 chars\n",
      "Text: C'est vraiment dommage que les formateurs présents dans cette salle ignorent autant la plupart des gens, sauf pour les cours particuliers. Lieu grand ...\n",
      "\n",
      "Label 0:\n",
      "Original length: 1028 chars\n",
      "Text: vraiment dommage coachs cette salle si mal ignorent plupart gens, sauf prennent cours particuliers. endroit grand bien équipé principalement marque te...\n",
      "\n",
      "Label 0:\n",
      "Original length: 355 chars\n",
      "Text: ne pourra jamais bénéficier du droit au sauna. à chaque fois une nouvelle objection. 21 heures ferme à 22 heures. ça prendra 30 minutes pour marcher, ...\n",
      "\n",
      "Label 0:\n",
      "Original length: 345 chars\n",
      "Text: ne pourra jamais bénéficier du droit au sauna. à chaque fois une nouvelle objection. 21h, fermeture à 22h. Il faudra 30 minutes de marche etc. Bon, ça...\n",
      "\n",
      "Label 0:\n",
      "Original length: 294 chars\n",
      "Text: malheur enregistrez cette salle de sport _exclamation_ _exclamation_ douche présence de moisissure, abonnés énormes très peu de fuites de matériel de ...\n",
      "\n",
      "Label 0:\n",
      "Original length: 275 chars\n",
      "Text: Malheur enregistrez cette salle de gym _exclamation_ _exclamation_ douche présence de moisissures, abonnés énormes, très peu de fuite de clim, tête do...\n",
      "\n",
      "Label 0:\n",
      "Original length: 75 chars\n",
      "Text: ascenseur toujours en panne _exclamation_ piscine sale. sinon tout va bien....\n",
      "\n",
      "Label 0:\n",
      "Original length: 75 chars\n",
      "Text: Ascenseur toujours en panne _exclamation_ piscine sale. Sinon tout va bien....\n"
     ]
    }
   ],
   "source": [
    "# Check augmented samples quality\n",
    "augmented_samples = df[df['is_aug'] == True].head(20)\n",
    "for i, row in augmented_samples.iterrows():\n",
    "    print(f\"\\nLabel {row['label']}:\")\n",
    "    print(f\"Original length: {len(row['text_clean'])} chars\")\n",
    "    print(f\"Text: {row['text_clean'][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 658 reviews\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"../data/cleaned/ml-methods-splits/augmented_simple/train_augmented.csv\"\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    print(f\"Error: File not found at {DATA_PATH}\")\n",
    "else:\n",
    "    # Load the data\n",
    "    df = pd.read_csv(DATA_PATH, encoding='utf-8-sig')\n",
    "    print(f\"Successfully loaded {len(df):,} reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 658 augmented reviews\n",
      "\n",
      "Initial state:\n",
      "Columns: ['text_clean', 'label', 'is_aug']\n",
      "Shape: (658, 3)\n",
      "Distribution:\n",
      "label\n",
      "2    257\n",
      "0    220\n",
      "1    181\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After removing empty rows: 658 samples\n",
      "\n",
      "Preprocessing augmented train set...\n",
      "Removed 0 empty rows after preprocessing.\n",
      "Removed 12 duplicate rows.\n",
      "FINAL CLEANED AUGMENTED DATA:\n",
      "Total samples: 646\n",
      "Class distribution:\n",
      "label\n",
      "2    257\n",
      "0    212\n",
      "1    177\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Saved cleaned augmented data to: ../data/cleaned/ml-methods-splits/augmented_simple/train_augmented_cleaned.csv\n",
      "SAMPLE CLEANED AUGMENTED TEXTS:\n",
      "\n",
      "Label 0 samples:\n",
      "  répondent jamais téléphone _exclamation_ allé magasin pendant heures ouverture fermés _exclamation_ ...\n",
      "  vraiment trop bruyant entend tout quartier autre côté rue normal salle équipée système insonorisatio...\n",
      "\n",
      "Label 1 samples:\n",
      "  lieu confirmé...\n",
      "  california gym is my go to place for great workout _exclamation_ the trainers are skilled and offer ...\n",
      "\n",
      "Label 2 samples:\n",
      "  possède toutes fonctionnalités dont besoin, exception sac boxe...\n",
      "  très bon service...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# DEEP CLEANING FOR AUGMENTED TRAIN SET\n",
    "\n",
    "# 1. Load your augmented data\n",
    "DATA_PATH = \"../data/cleaned/ml-methods-splits/augmented_simple/train_augmented.csv\"\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    print(f\"Error: File not found at {DATA_PATH}\")\n",
    "else:\n",
    "    # Load the data\n",
    "    df_augmented = pd.read_csv(DATA_PATH, encoding='utf-8-sig')\n",
    "    print(f\"Successfully loaded {len(df_augmented):,} augmented reviews\")\n",
    "\n",
    "# Check initial state\n",
    "print(\"\\nInitial state:\")\n",
    "print(f\"Columns: {df_augmented.columns.tolist()}\")\n",
    "print(f\"Shape: {df_augmented.shape}\")\n",
    "print(f\"Distribution:\\n{df_augmented['label'].value_counts()}\")\n",
    "\n",
    "# 2. Remove 'is_aug' column and keep only text_clean and label\n",
    "df_augmented = df_augmented[['text_clean', 'label']]\n",
    "\n",
    "# 3. Basic cleaning: remove NaN and empty strings\n",
    "df_augmented = df_augmented.dropna(subset=['text_clean'])\n",
    "df_augmented = df_augmented[df_augmented['text_clean'].str.strip() != '']\n",
    "print(f\"\\nAfter removing empty rows: {len(df_augmented):,} samples\")\n",
    "\n",
    "# 4. Define cleaning functions \n",
    "def clean_text_ml(text):\n",
    "    \"\"\"Clean text for TF-IDF / classical ML.\"\"\"\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove unwanted chars but keep basic punctuation\n",
    "    text = re.sub(r\"[^\\w\\sàâäéèêëîïôöùûüç.!?,;:]\", \" \", text)\n",
    "    \n",
    "    # mark punctuation (important for sentiment)\n",
    "    text = re.sub(r\"(!+)\", \" _exclamation_ \", text)\n",
    "    text = re.sub(r\"(\\?+)\", \" _question_ \", text)\n",
    "    \n",
    "    # remove URLs, mentions, hashtags\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \" \", text)\n",
    "    text = re.sub(r\"@\\w+\", \" \", text)\n",
    "    text = re.sub(r\"#\\w+\", \" \", text)\n",
    "    \n",
    "    # normalize spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Download French stopwords if not already available\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "french_stop = set(stopwords.words('french'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Remove French stopwords.\"\"\"\n",
    "    words = text.split()\n",
    "    filtered_words = []\n",
    "    for w in words:\n",
    "        # Keep sentiment tokens and special markers\n",
    "        if w.startswith('_emoji_') or w.startswith('_exclamation_') or w.startswith('_question_'):\n",
    "            filtered_words.append(w)\n",
    "        elif w not in french_stop and len(w) > 1:\n",
    "            filtered_words.append(w)\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    \"\"\"Apply full preprocessing pipeline.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Step 1: clean text\n",
    "    df[\"text_clean\"] = df[\"text_clean\"].apply(clean_text_ml)\n",
    "    \n",
    "    # Step 2: remove stopwords\n",
    "    df[\"text_clean\"] = df[\"text_clean\"].apply(remove_stopwords)\n",
    "    \n",
    "    # Step 3: remove empty rows after cleaning\n",
    "    before = df.shape[0]\n",
    "    df[\"text_clean\"] = df[\"text_clean\"].replace(\"\", np.nan)\n",
    "    df = df.dropna(subset=[\"text_clean\"])\n",
    "    after = df.shape[0]\n",
    "    print(f\"Removed {before - after} empty rows after preprocessing.\")\n",
    "    \n",
    "    # Step 4: remove exact duplicates\n",
    "    before_dedup = df.shape[0]\n",
    "    df = df.drop_duplicates(subset=[\"text_clean\"])\n",
    "    after_dedup = df.shape[0]\n",
    "    print(f\"Removed {before_dedup - after_dedup} duplicate rows.\")\n",
    "    \n",
    "    # Step 5: ensure we only have 2 columns\n",
    "    df = df[[\"text_clean\", \"label\"]]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 5. Apply preprocessing to augmented data\n",
    "print(\"\\nPreprocessing augmented train set...\")\n",
    "df_augmented_cleaned = preprocess_dataframe(df_augmented)\n",
    "\n",
    "# 6. Check final distribution\n",
    "print(\"FINAL CLEANED AUGMENTED DATA:\")\n",
    "print(f\"Total samples: {len(df_augmented_cleaned):,}\")\n",
    "print(f\"Class distribution:\")\n",
    "print(df_augmented_cleaned['label'].value_counts())\n",
    "\n",
    "# 7. Save the cleaned augmented data\n",
    "output_path = \"../data/cleaned/ml-methods-splits/augmented_simple/train_augmented_cleaned.csv\"\n",
    "df_augmented_cleaned.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "print(f\"\\nSaved cleaned augmented data to: {output_path}\")\n",
    "\n",
    "# 8. Show some samples\n",
    "print(\"SAMPLE CLEANED AUGMENTED TEXTS:\")\n",
    "\n",
    "for label in [0, 1, 2]:\n",
    "    samples = df_augmented_cleaned[df_augmented_cleaned['label'] == label].head(2)\n",
    "    print(f\"\\nLabel {label} samples:\")\n",
    "    for idx, row in samples.iterrows():\n",
    "        print(f\"  {row['text_clean'][:100]}...\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8943044,
     "sourceId": 14047499,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
